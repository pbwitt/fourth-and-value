{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc751db",
   "metadata": {},
   "source": [
    "# Week 2 â€” Build Actuals âžœ Grade Model Performance (Allâ€‘inâ€‘One, v2) ðŸš€\n",
    "\n",
    "This version is **robust to missing/renamed `side` columns** in your props.  \n",
    "It will derive a normalized `side` (Over/Under/Yes/No) from any of:\n",
    "- `side`, `bet_side`, `ou_side`, `yes_no_side`, or by parsing `bet` text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa640ab-71b5-4cef-8a1a-f920cb131c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[props] (2673, 24)\n",
      "[props expanded] (2974, 25)\n",
      "[stats filtered] (970, 114)\n",
      "[join] common_markets not available: No module named 'common_markets'\n",
      "[join] using local fallback normalizer\n",
      "[merge] merged rows: 2974  matched: 156\n",
      "\n",
      "[Week2 overall]\n",
      "  rows    : 156\n",
      "  hit rate: 0.5\n",
      "  brier   : 0.32772987664122855\n",
      "  logloss : 0.8793457747230159\n",
      "[write] grades  -> /Users/pwitt/fourth-and-value/notebooks/data/eval/grades_week2.csv (rows=156)\n",
      "[write] markets -> /Users/pwitt/fourth-and-value/notebooks/data/eval/market_perf_week2.csv (rows=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_std</th>\n",
       "      <th>n</th>\n",
       "      <th>hit</th>\n",
       "      <th>brier</th>\n",
       "      <th>logloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anytime_td</td>\n",
       "      <td>156</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32773</td>\n",
       "      <td>0.879346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   market_std    n  hit    brier   logloss\n",
       "0  anytime_td  156  0.5  0.32773  0.879346"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Week 2: props vs actuals grading (join via player_display_name) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, re, sys\n",
    "\n",
    "\n",
    "\n",
    "SEASON, WEEK = 2025, 2\n",
    "BASE = Path.cwd()\n",
    "props_file = BASE / f\"props_with_model_week{WEEK}.csv\"\n",
    "stats_file = BASE / \"weekly_player_stats_2025.parquet\"\n",
    "\n",
    "# 1) Load props\n",
    "dfp = pd.read_csv(props_file)\n",
    "print(\"[props]\", dfp.shape)\n",
    "\n",
    "# 2) Expand explicit sides\n",
    "OU = {\n",
    "    \"rush_yds\",\"rushing_yards\",\n",
    "    \"recv_yds\",\"reception_yds\",\"receiving_yards\",\n",
    "    \"pass_yds\",\"passing_yards\",\n",
    "    \"receptions\",\"rush_attempts\",\"completions\",\"attempts\",\n",
    "}\n",
    "YESNO = {\"anytime_td\"}\n",
    "\n",
    "rows = []\n",
    "for _, r in dfp.iterrows():\n",
    "    m = str(r[\"market_std\"]).lower()\n",
    "    if m in OU and pd.notna(r.get(\"point\")):\n",
    "        for side in (\"over\",\"under\"):\n",
    "            x = r.copy(); x[\"side\"] = side; rows.append(x)\n",
    "    elif m in YESNO:\n",
    "        for side in (\"yes\",\"no\"):\n",
    "            x = r.copy(); x[\"side\"] = side; rows.append(x)\n",
    "    # skip exotics for grading\n",
    "dfp = pd.DataFrame(rows).reset_index(drop=True)\n",
    "print(\"[props expanded]\", dfp.shape)\n",
    "\n",
    "# 3) Load stats (parquet)\n",
    "dfs = pd.read_parquet(stats_file)\n",
    "dfs = dfs[(dfs[\"season\"]==SEASON) & (dfs[\"week\"]==WEEK)]\n",
    "print(\"[stats filtered]\", dfs.shape)\n",
    "\n",
    "# 4) Name normalizer (try common_markets; fallback locally)\n",
    "scripts_dir = BASE / \"scripts\"\n",
    "if str(scripts_dir) not in sys.path:\n",
    "    sys.path.append(str(scripts_dir))\n",
    "\n",
    "def _fallback_std_name(s):\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)): return \"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9 ]+\", \"\", s)          # keep letters/digits/spaces\n",
    "    s = \" \".join(t for t in s.split() if t not in {\"jr\",\"sr\",\"ii\",\"iii\",\"iv\",\"v\"})\n",
    "    return s.replace(\" \", \"\")                  # collapse spaces\n",
    "\n",
    "std_name = None\n",
    "try:\n",
    "    import common_markets as cm\n",
    "    for cand in (\"std_player_name\",\"std_name\",\"normalize_name\",\"name_std\",\"norm_name\"):\n",
    "        if hasattr(cm, cand):\n",
    "            std_name = getattr(cm, cand); print(f\"[join] using common_markets.{cand}\")\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(\"[join] common_markets not available:\", e)\n",
    "if std_name is None:\n",
    "    std_name = _fallback_std_name\n",
    "    print(\"[join] using local fallback normalizer\")\n",
    "\n",
    "# 5) Build canonical name_std on BOTH sides\n",
    "# props: prefer existing name_std if present; otherwise from 'name' or 'player'\n",
    "source_col = \"name_std\" if \"name_std\" in dfp.columns else (\"name\" if \"name\" in dfp.columns else \"player\")\n",
    "dfp[\"name_std\"] = dfp[source_col].apply(std_name)\n",
    "\n",
    "# stats: IMPORTANT â€” use player_display_name (full name), not player_name (abbr)\n",
    "dfs[\"name_std\"] = dfs[\"player_display_name\"].apply(std_name)\n",
    "\n",
    "# 6) Merge on canonical name\n",
    "dfm = dfp.merge(dfs, on=\"name_std\", how=\"left\", suffixes=(\"\",\"_stat\"))\n",
    "matched = dfm[\"player_id\"].notna().sum() if \"player_id\" in dfm.columns else dfm[\"team\"].notna().sum()\n",
    "print(f\"[merge] merged rows: {len(dfm)}  matched: {matched}\")\n",
    "\n",
    "# 7) Compute actual_value per market\n",
    "MAP = {\n",
    "    # Passing\n",
    "    \"pass_yds\":\"passing_yards\",\"passing_yards\":\"passing_yards\",\n",
    "    \"pass_tds\":\"passing_tds\",\"passing_tds\":\"passing_tds\",\n",
    "    \"pass_ints\":\"passing_interceptions\",\"pass_interceptions\":\"passing_interceptions\",\n",
    "    \"completions\":\"completions\",\"pass_cmp\":\"completions\",\n",
    "    \"attempts\":\"attempts\",\"pass_att\":\"attempts\",\n",
    "    # Rushing\n",
    "    \"rush_yds\":\"rushing_yards\",\"rushing_yards\":\"rushing_yards\",\n",
    "    \"rush_attempts\":\"carries\",\"carries\":\"carries\",\n",
    "    \"rushing_tds\":\"rushing_tds\",\n",
    "    # Receiving\n",
    "    \"recv_yds\":\"receiving_yards\",\"reception_yds\":\"receiving_yards\",\"receiving_yards\":\"receiving_yards\",\n",
    "    \"receptions\":\"receptions\",\"receiving_tds\":\"receiving_tds\",\n",
    "}\n",
    "def actual_value(row):\n",
    "    m = str(row[\"market_std\"]).lower()\n",
    "    if m in YESNO:\n",
    "        return float((row.get(\"rushing_tds\",0) or 0) + (row.get(\"receiving_tds\",0) or 0))\n",
    "    col = MAP.get(m)\n",
    "    return float(row[col]) if col and col in row and pd.notna(row[col]) else np.nan\n",
    "\n",
    "dfm[\"actual_value\"] = dfm.apply(actual_value, axis=1)\n",
    "\n",
    "# 8) Decide result\n",
    "def decide(side, actual, point):\n",
    "    if pd.isna(actual): return np.nan\n",
    "    s = str(side).lower()\n",
    "    if s==\"yes\": return 1.0 if actual>0 else 0.0\n",
    "    if s==\"no\":  return 1.0 if actual==0 else 0.0\n",
    "    if pd.isna(point): return np.nan\n",
    "    return 1.0 if (s==\"over\" and actual>=point) or (s==\"under\" and actual<=point) else 0.0\n",
    "\n",
    "dfm[\"result\"] = dfm.apply(lambda r: decide(r[\"side\"], r[\"actual_value\"], r.get(\"point\")), axis=1)\n",
    "\n",
    "# 9) Save actuals\n",
    "##Zactuals = dfm.loc[dfm[\"actual_value\"].notna(), [\"player_key\",\"name\",\"market_std\",\"side\",\"point\",\"actual_value\",\"result\"]]\n",
    "#actuals.to_csv(actuals_out, index=False)\n",
    "#print(f\"[write] actuals -> {actuals_out} rows={len(actuals)}\")\n",
    "\n",
    "# 10) Metrics (overall + per-market)\n",
    "p_model = \"model_prob\"\n",
    "eps = 1e-9\n",
    "mask = dfm[\"result\"].notna() & dfm[p_model].notna()\n",
    "graded = dfm.loc[mask].copy()\n",
    "\n",
    "if len(graded)==0:\n",
    "    print(\"[grade] no graded rows â€” check name join coverage above\")\n",
    "else:\n",
    "    graded[\"brier\"] = (graded[p_model] - graded[\"result\"])**2\n",
    "    p = np.clip(graded[p_model].astype(float), eps, 1-eps)\n",
    "    graded[\"logloss\"] = -(graded[\"result\"]*np.log(p) + (1-graded[\"result\"])*np.log(1-p))\n",
    "\n",
    "    # Overall\n",
    "    print(\"\\n[Week2 overall]\")\n",
    "    print(\"  rows    :\", len(graded))\n",
    "    print(\"  hit rate:\", graded[\"result\"].mean())\n",
    "    print(\"  brier   :\", graded[\"brier\"].mean())\n",
    "    print(\"  logloss :\", graded[\"logloss\"].mean())\n",
    "\n",
    "    # Per-market summary\n",
    "    def safemean(s): return pd.to_numeric(s, errors=\"coerce\").dropna().mean()\n",
    "    market_perf = (graded\n",
    "        .groupby(\"market_std\", dropna=False)\n",
    "        .agg(n=(\"result\",\"count\"),\n",
    "             hit=(\"result\",\"mean\"),\n",
    "             brier=(\"brier\", safemean),\n",
    "             logloss=(\"logloss\", safemean))\n",
    "        .reset_index()\n",
    "        .sort_values([\"n\",\"hit\"], ascending=[False,False])\n",
    "    )\n",
    "\n",
    "    # Write grades + per-market\n",
    "    grades_out = BASE / f\"data/eval/grades_week{WEEK}.csv\"\n",
    "    market_out = BASE / f\"data/eval/market_perf_week{WEEK}.csv\"\n",
    "    grades_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    graded.to_csv(grades_out, index=False)\n",
    "    market_perf.to_csv(market_out, index=False)\n",
    "    print(f\"[write] grades  -> {grades_out} (rows={len(graded)})\")\n",
    "    print(f\"[write] markets -> {market_out} (rows={len(market_perf)})\")\n",
    "\n",
    "    # Quick glance\n",
    "    display(market_perf.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a14af3-2dec-42c1-bb46-e3c9d45f7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEASON, WEEK = 2025, 2\n",
    "BASE = Path.cwd()\n",
    "props_file = BASE / f\"props_with_model_week{WEEK}.csv\"\n",
    "stats_file = BASE / \"weekly_player_stats_2025.parquet\"\n",
    "\n",
    "# 1) Load props\n",
    "dfp = pd.read_csv(props_file)\n",
    "print(\"[props]\", dfp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f587eef-f01b-445a-a9f1-f4322a86bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.player_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ceafb-9aef-4df9-9f3f-d08a015a2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607674f1-bb87-49d7-bf38-22f9720064f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.player_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90775d0e-6885-4eba-a333-ab7860f442bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- 0) Minimal helpers (use your scripts/common_markets if you prefer) ---\n",
    "def _std_name(x: str) -> str:\n",
    "    if pd.isna(x): return \"\"\n",
    "    x = x.lower()\n",
    "    x = re.sub(r\"[^a-z0-9\\s]\", \"\", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "# Market aliases â†’ a compact, opinionated set that matches nflverse columns below\n",
    "ALIASES = {\n",
    "    # yards\n",
    "    \"player_pass_yds\": \"pass_yds\", \"pass_yds\": \"pass_yds\",\n",
    "    \"player_rush_yds\": \"rush_yds\", \"rush_yds\": \"rush_yds\",\n",
    "    \"player_rec_yds\": \"rec_yds\",   \"rec_yds\": \"rec_yds\", \"reception_yds\": \"rec_yds\",\n",
    "    # counts\n",
    "    \"receptions\": \"receptions\", \"player_receptions\": \"receptions\",\n",
    "    \"pass_attempts\": \"pass_attempts\", \"player_pass_attempts\": \"pass_attempts\",\n",
    "    \"pass_completions\": \"pass_completions\", \"player_pass_completions\": \"pass_completions\",\n",
    "    \"rush_attempts\": \"rush_attempts\", \"carries\": \"rush_attempts\",\n",
    "    # scoring / turnovers\n",
    "    \"pass_tds\": \"pass_tds\", \"player_pass_tds\": \"pass_tds\",\n",
    "    \"pass_interceptions\": \"pass_ints\", \"interceptions\": \"pass_ints\",\n",
    "    \"anytime_td\": \"anytime_td\",\n",
    "    # longest\n",
    "    \"rush_longest\": \"rush_long\", \"player_rush_longest\": \"rush_long\",\n",
    "    \"reception_longest\": \"rec_long\", \"player_reception_longest\": \"rec_long\",\n",
    "}\n",
    "\n",
    "def _std_market(x: str) -> str:\n",
    "    if pd.isna(x): return \"\"\n",
    "    y = x.strip().lower()\n",
    "    return ALIASES.get(y, y)\n",
    "\n",
    "def _first_existing(colnames, df):\n",
    "    for c in colnames:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# --- 1) Normalize keys on both sides ---\n",
    "dfp = dfp.copy()\n",
    "dfs = dfs.copy()\n",
    "\n",
    "dfp[\"name_std\"] = dfp[_first_existing([\"player\",\"name\",\"Player\"], dfp)].map(_std_name)\n",
    "dfp[\"market_std\"] = dfp[_first_existing([\"market\",\"Market\"], dfp)].map(_std_market)\n",
    "\n",
    "dfs[\"name_std\"] = dfs[_first_existing([\"player\",\"name\",\"player_name\"], dfs)].map(_std_name)\n",
    "\n",
    "# --- 2) Choose line & side columns (robust to schema differences) ---\n",
    "line_col = _first_existing([\"line\",\"point\",\"line_disp\",\"points\",\"handicap\",\"total\"], dfp)\n",
    "if line_col is None:\n",
    "    dfp[\"line\"] = np.nan\n",
    "else:\n",
    "    dfp[\"line\"] = pd.to_numeric(dfp[line_col], errors=\"coerce\")\n",
    "\n",
    "# Side can be Over/Under or Yes/No; try to derive if missing\n",
    "if \"side\" not in dfp.columns:\n",
    "    bet_col = _first_existing([\"bet\",\"Bet\",\"wager\",\"selection\"], dfp)\n",
    "    if bet_col:\n",
    "        dfp[\"side\"] = dfp[bet_col].str.extract(r\"^(Over|Under|Yes|No)\", expand=False)\n",
    "    else:\n",
    "        dfp[\"side\"] = np.nan\n",
    "\n",
    "# --- 3) Map df markets -> dfs stat columns (DEFENSIVE) ---\n",
    "def pick_col(df, *cands):\n",
    "    for c in cands:\n",
    "        if c and c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "MARKET_TO_COL = {\n",
    "    # yards\n",
    "    \"pass_yds\":      pick_col(dfs, \"passing_yards\", \"pass_yards\", \"pass_yds\"),\n",
    "    \"rush_yds\":      pick_col(dfs, \"rushing_yards\", \"rush_yards\", \"rush_yds\"),\n",
    "    \"rec_yds\":       pick_col(dfs, \"receiving_yards\", \"rec_yards\", \"rec_yds\", \"reception_yds\"),\n",
    "    # counts\n",
    "    \"receptions\":    pick_col(dfs, \"receptions\", \"recs\"),\n",
    "    \"pass_attempts\": pick_col(dfs, \"passing_attempts\", \"pass_attempts\", \"attempts\"),\n",
    "    \"pass_completions\": pick_col(dfs, \"passing_completions\", \"pass_completions\", \"completions\"),\n",
    "    \"rush_attempts\": pick_col(dfs, \"rushing_attempts\", \"rush_attempts\", \"carries\"),\n",
    "    # scoring / turnovers\n",
    "    \"pass_tds\":      pick_col(dfs, \"passing_tds\", \"pass_tds\"),\n",
    "    \"pass_ints\":     pick_col(dfs, \"interceptions\", \"passing_interceptions\", \"pass_ints\", \"ints\"),\n",
    "    # longest\n",
    "    \"rush_long\":     pick_col(dfs, \"rushing_long\", \"rush_long\", \"long_rush\"),\n",
    "    \"rec_long\":      pick_col(dfs, \"receiving_long\", \"rec_long\", \"long_rec\"),\n",
    "    # anytime_td handled specially below\n",
    "}\n",
    "\n",
    "# TD columns for anytime TD\n",
    "td_candidates = [\"total_tds\",\"rushing_tds\",\"receiving_tds\",\"kick_return_tds\",\"punt_return_tds\",\n",
    "                 \"defensive_tds\",\"special_teams_tds\"]\n",
    "td_cols = [c for c in td_candidates if c in dfs.columns]\n",
    "\n",
    "# Build the exact set of columns we will slice from dfs (only those that exist)\n",
    "used_stat_cols = {\"name_std\"}\n",
    "used_stat_cols.update([c for c in MARKET_TO_COL.values() if c is not None])\n",
    "used_stat_cols.update(td_cols)\n",
    "\n",
    "dfs_slice = dfs[list(used_stat_cols)].copy()\n",
    "\n",
    "# Helpful diagnostics\n",
    "missing_map = {m: col for m, col in MARKET_TO_COL.items() if col is None}\n",
    "if missing_map:\n",
    "    print(\"[warn] missing stat columns for these markets (no matching column in dfs):\", missing_map)\n",
    "\n",
    "\n",
    "# --- 4) Merge (player-level, same week) ---\n",
    "merged = (\n",
    "    dfp.merge(dfs_slice, on=\"name_std\", how=\"left\", suffixes=(\"\",\"_s\"))\n",
    ")\n",
    "\n",
    "print(f\"[merge] matched player rows: {(~merged[dfs_slice.columns.difference(['name_std'])].isna()).any(axis=1).sum()} / {len(merged)}\")\n",
    "\n",
    "# --- 5) Compute actual_value per row (vectorized by market_std) ---\n",
    "merged[\"actual_value\"] = np.nan\n",
    "\n",
    "for m, col in MARKET_TO_COL.items():\n",
    "    if isinstance(col, str) and col in merged.columns:\n",
    "        sel = merged[\"market_std\"].eq(m)\n",
    "        merged.loc[sel, \"actual_value\"] = pd.to_numeric(merged.loc[sel, col], errors=\"coerce\")\n",
    "\n",
    "# Anytime TD (binary)\n",
    "if \"anytime_td\" in merged[\"market_std\"].unique():\n",
    "    # prefer total_tds if present; else sum rushing+receiving+returns\n",
    "    if \"total_tds\" in merged.columns:\n",
    "        td_any = (pd.to_numeric(merged[\"total_tds\"], errors=\"coerce\") > 0).astype(int)\n",
    "    else:\n",
    "        parts = [pd.to_numeric(merged.get(c, 0), errors=\"coerce\").fillna(0) for c in td_cols]\n",
    "        td_any = (np.sum(parts, axis=0) > 0).astype(int)\n",
    "    merged.loc[merged[\"market_std\"].eq(\"anytime_td\"), \"actual_value\"] = td_any\n",
    "\n",
    "# --- 6) Derive actual_side (handles O/U + Yes/No + Push) ---\n",
    "OU_MARKETS = {\"pass_yds\",\"rush_yds\",\"rec_yds\",\"receptions\",\n",
    "              \"pass_attempts\",\"pass_completions\",\"rush_attempts\",\n",
    "              \"pass_tds\",\"pass_ints\",\"rush_long\",\"rec_long\"}\n",
    "\n",
    "merged[\"actual_side\"] = np.nan\n",
    "is_ou = merged[\"market_std\"].isin(OU_MARKETS) & merged[\"line\"].notna() & merged[\"actual_value\"].notna()\n",
    "merged.loc[is_ou & (merged[\"actual_value\"] > merged[\"line\"]), \"actual_side\"] = \"Over\"\n",
    "merged.loc[is_ou & (merged[\"actual_value\"] < merged[\"line\"]), \"actual_side\"] = \"Under\"\n",
    "merged.loc[is_ou & (merged[\"actual_value\"] == merged[\"line\"]), \"actual_side\"] = \"Push\"  # keep pushes explicit\n",
    "\n",
    "is_any = merged[\"market_std\"].eq(\"anytime_td\") & merged[\"actual_value\"].notna()\n",
    "merged.loc[is_any, \"actual_side\"] = np.where(merged.loc[is_any, \"actual_value\"].astype(int) > 0, \"Yes\", \"No\")\n",
    "\n",
    "# --- 7) Quick coverage + sanity prints ---\n",
    "def _c(n): return f\"{n:,}\"\n",
    "print(\"[coverage]\")\n",
    "print(\"  rows total        :\", _c(len(merged)))\n",
    "print(\"  rows w/ actualval :\", _c(merged[\"actual_value\"].notna().sum()))\n",
    "print(\"  rows w/ actualside:\", _c(merged[\"actual_side\"].notna().sum()))\n",
    "print(\"  pushes (O/U)      :\", _c((merged[\"actual_side\"]==\"Push\").sum()))\n",
    "\n",
    "# Unmapped markets you might want to add to ALIASES/MARKET_TO_COL\n",
    "unmapped = (merged.loc[merged[\"actual_value\"].isna(), \"market_std\"]\n",
    "            .value_counts().head(15))\n",
    "print(\"\\n[top unmapped markets]\")\n",
    "print(unmapped)\n",
    "\n",
    "# --- 8) Optional: compute hit flag (ignoring NaN & Push) ---\n",
    "has_side = merged[\"side\"].notna() & merged[\"actual_side\"].notna()\n",
    "not_push = merged[\"actual_side\"] != \"Push\"\n",
    "merged[\"hit\"] = np.where(has_side & not_push & (merged[\"side\"] == merged[\"actual_side\"]), 1,\n",
    "                  np.where(has_side & not_push, 0, np.nan))\n",
    "\n",
    "print(\"\\n[hit-rate quick check]\")\n",
    "mask = merged[\"hit\"].notna()\n",
    "if mask.any():\n",
    "    print(f\"  hit-rate: {merged.loc[mask,'hit'].mean():.3f} on {int(mask.sum())} graded\")\n",
    "else:\n",
    "    print(\"  no graded rows yet (likely missing 'side' or mapping)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c11431-0c47-45f2-8696-65c702b81d88",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/pwitt/fourth-and-value/notebooks/eekly_player_stats_2025.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m BASE = Path.cwd()\n\u001b[32m      5\u001b[39m dfp = pd.read_csv(BASE/\u001b[33m\"\u001b[39m\u001b[33mprops_with_model_week2.csv\u001b[39m\u001b[33m\"\u001b[39m)   \u001b[38;5;66;03m# projections\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dfs = pd.read_parquet(BASE/\u001b[33m\"\u001b[39m\u001b[33meekly_player_stats_2025.parquet\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# nflverse weekly; filter week=2 below\u001b[39;00m\n\u001b[32m      7\u001b[39m dfs = dfs.query(\u001b[33m\"\u001b[39m\u001b[33mseason==2025 and week==2\u001b[39m\u001b[33m\"\u001b[39m).copy()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# --- normalizers (use your scripts/common_markets if available) ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/nfl2025/lib/python3.11/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m impl.read(\n\u001b[32m    670\u001b[39m     path,\n\u001b[32m    671\u001b[39m     columns=columns,\n\u001b[32m    672\u001b[39m     filters=filters,\n\u001b[32m    673\u001b[39m     storage_options=storage_options,\n\u001b[32m    674\u001b[39m     use_nullable_dtypes=use_nullable_dtypes,\n\u001b[32m    675\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    676\u001b[39m     filesystem=filesystem,\n\u001b[32m    677\u001b[39m     **kwargs,\n\u001b[32m    678\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/nfl2025/lib/python3.11/site-packages/pandas/io/parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/nfl2025/lib/python3.11/site-packages/pandas/io/parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = get_handle(\n\u001b[32m    142\u001b[39m         path_or_handle, mode, is_text=\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options=storage_options\n\u001b[32m    143\u001b[39m     )\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/nfl2025/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/pwitt/fourth-and-value/notebooks/eekly_player_stats_2025.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path.cwd()\n",
    "dfp = pd.read_csv(BASE/\"props_with_model_week2.csv\")   # projections\n",
    "dfs = pd.read_parquet(BASE/\"eekly_player_stats_2025.parquet\")  # nflverse weekly; filter week=2 below\n",
    "dfs = dfs.query(\"season==2025 and week==2\").copy()\n",
    "\n",
    "# --- normalizers (use your scripts/common_markets if available) ---\n",
    "try:\n",
    "    from scripts.common_markets import std_market as _std_market, std_name as _std_name\n",
    "except Exception:\n",
    "    def _std_name(x: str) -> str:\n",
    "        if pd.isna(x): return \"\"\n",
    "        x = x.lower()\n",
    "        x = re.sub(r\"[^a-z0-9\\s]\", \"\", x)\n",
    "        return re.sub(r\"\\s+\", \" \", x).strip()\n",
    "\n",
    "    ALIASES = {\n",
    "        \"player_pass_yds\":\"pass_yds\",\"pass_yds\":\"pass_yds\",\n",
    "        \"player_rush_yds\":\"rush_yds\",\"rush_yds\":\"rush_yds\",\n",
    "        \"player_rec_yds\":\"rec_yds\",\"rec_yds\":\"rec_yds\",\"reception_yds\":\"rec_yds\",\n",
    "        \"receptions\":\"receptions\",\"player_receptions\":\"receptions\",\n",
    "        \"pass_attempts\":\"pass_attempts\",\"player_pass_attempts\":\"pass_attempts\",\n",
    "        \"pass_completions\":\"pass_completions\",\"player_pass_completions\":\"pass_completions\",\n",
    "        \"rush_attempts\":\"rush_attempts\",\"carries\":\"rush_attempts\",\n",
    "        \"pass_tds\":\"pass_tds\",\"player_pass_tds\":\"pass_tds\",\n",
    "        \"pass_interceptions\":\"pass_ints\",\"interceptions\":\"pass_ints\",\n",
    "        \"anytime_td\":\"anytime_td\",\n",
    "        \"rush_longest\":\"rush_long\",\"player_rush_longest\":\"rush_long\",\n",
    "        \"reception_longest\":\"rec_long\",\"player_reception_longest\":\"rec_long\",\n",
    "    }\n",
    "    def _std_market(x: str) -> str:\n",
    "        if pd.isna(x): return \"\"\n",
    "        y = x.strip().lower()\n",
    "        return ALIASES.get(y, y)\n",
    "\n",
    "# --- keys ---\n",
    "dfp = dfp.copy()\n",
    "dfp[\"name_std\"] = dfp[[\"player\",\"name\"]].ffill(axis=1).iloc[:,0].map(_std_name)\n",
    "dfp[\"market_std\"] = dfp[[\"market\",\"Market\"]].ffill(axis=1).iloc[:,0].map(_std_market)\n",
    "# pick a line column & side column\n",
    "line_col = next((c for c in [\"line\",\"point\",\"line_disp\",\"points\",\"handicap\",\"total\"] if c in dfp.columns), None)\n",
    "dfp[\"line\"] = pd.to_numeric(dfp[line_col], errors=\"coerce\") if line_col else np.nan\n",
    "if \"side\" not in dfp.columns:\n",
    "    guess = next((c for c in [\"bet\",\"Bet\",\"selection\"] if c in dfp.columns), None)\n",
    "    dfp[\"side\"] = dfp[guess].str.extract(r\"^(Over|Under|Yes|No)\", expand=False) if guess else np.nan\n",
    "\n",
    "# dfs keys\n",
    "dfs = dfs.copy()\n",
    "name_col = next((c for c in [\"player\",\"player_name\",\"name\"] if c in dfs.columns), None)\n",
    "dfs[\"name_std\"] = dfs[name_col].map(_std_name)\n",
    "\n",
    "# --- map markets to stat columns (defensive against schema diffs) ---\n",
    "def pick(df, *cands):\n",
    "    for c in cands:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "M2C = {\n",
    "    \"pass_yds\":      pick(dfs, \"passing_yards\",\"pass_yards\"),\n",
    "    \"rush_yds\":      pick(dfs, \"rushing_yards\",\"rush_yards\"),\n",
    "    \"rec_yds\":       pick(dfs, \"receiving_yards\",\"rec_yards\"),\n",
    "    \"receptions\":    pick(dfs, \"receptions\"),\n",
    "    \"pass_attempts\": pick(dfs, \"passing_attempts\"),\n",
    "    \"pass_completions\": pick(dfs, \"passing_completions\"),\n",
    "    \"rush_attempts\": pick(dfs, \"rush_attempts\",\"rushing_attempts\"),\n",
    "    \"pass_tds\":      pick(dfs, \"passing_tds\"),\n",
    "    \"pass_ints\":     pick(dfs, \"interceptions\",\"passing_interceptions\"),\n",
    "    \"rush_long\":     pick(dfs, \"rushing_long\"),\n",
    "    \"rec_long\":      pick(dfs, \"receiving_long\"),\n",
    "}\n",
    "\n",
    "td_parts = [c for c in [\"total_tds\",\"rushing_tds\",\"receiving_tds\",\"kick_return_tds\",\"punt_return_tds\",\n",
    "                        \"defensive_tds\",\"special_teams_tds\"] if c in dfs.columns]\n",
    "\n",
    "dfs_slice_cols = {\"name_std\"} | {c for c in M2C.values() if c} | set(td_parts)\n",
    "dfs_slice = dfs[list(dfs_slice_cols)].copy()\n",
    "\n",
    "# --- merge ---\n",
    "merged = dfp.merge(dfs_slice, on=\"name_std\", how=\"left\", suffixes=(\"\",\"_s\"))\n",
    "print(f\"[merge] rows: {len(merged):,}\")\n",
    "\n",
    "# --- actual_value per market ---\n",
    "merged[\"actual_value\"] = np.nan\n",
    "for m, col in M2C.items():\n",
    "    if col:\n",
    "        sel = merged[\"market_std\"].eq(m)\n",
    "        merged.loc[sel, \"actual_value\"] = pd.to_numeric(merged.loc[sel, col], errors=\"coerce\")\n",
    "\n",
    "# anytime TD (binary yes/no)\n",
    "if \"anytime_td\" in merged[\"market_std\"].unique():\n",
    "    if \"total_tds\" in merged.columns:\n",
    "        td_any = (pd.to_numeric(merged[\"total_tds\"], errors=\"coerce\") > 0).astype(int)\n",
    "    else:\n",
    "        parts = [pd.to_numeric(merged.get(c, 0), errors=\"coerce\").fillna(0) for c in td_parts]\n",
    "        td_any = (np.sum(parts, axis=0) > 0).astype(int)\n",
    "    merged.loc[merged[\"market_std\"].eq(\"anytime_td\"), \"actual_value\"] = td_any\n",
    "\n",
    "# --- actual_side (O/U & Yes/No), keep pushes explicit ---\n",
    "OU = {\"pass_yds\",\"rush_yds\",\"rec_yds\",\"receptions\",\"pass_attempts\",\"pass_completions\",\n",
    "      \"rush_attempts\",\"pass_tds\",\"pass_ints\",\"rush_long\",\"rec_long\"}\n",
    "merged[\"actual_side\"] = np.nan\n",
    "ou_mask = merged[\"market_std\"].isin(OU) & merged[\"line\"].notna() & merged[\"actual_value\"].notna()\n",
    "merged.loc[ou_mask & (merged[\"actual_value\"] > merged[\"line\"]), \"actual_side\"] = \"Over\"\n",
    "merged.loc[ou_mask & (merged[\"actual_value\"] < merged[\"line\"]), \"actual_side\"] = \"Under\"\n",
    "merged.loc[ou_mask & (merged[\"actual_value\"] == merged[\"line\"]), \"actual_side\"] = \"Push\"\n",
    "\n",
    "any_mask = merged[\"market_std\"].eq(\"anytime_td\") & merged[\"actual_value\"].notna()\n",
    "merged.loc[any_mask, \"actual_side\"] = np.where(merged.loc[any_mask,\"actual_value\"].astype(int)>0, \"Yes\", \"No\")\n",
    "\n",
    "# --- quick diagnostics ---\n",
    "print(\"[coverage]\")\n",
    "print(\"  with actual_value :\", int(merged[\"actual_value\"].notna().sum()))\n",
    "print(\"  with actual_side  :\", int(merged[\"actual_side\"].notna().sum()))\n",
    "missing = (merged.loc[merged[\"actual_value\"].isna(),\"market_std\"].value_counts().head(10))\n",
    "if len(missing): print(\"[unmapped top]\"); print(missing)\n",
    "\n",
    "# --- grade (ignore pushes & rows without a declared side) ---\n",
    "mask = merged[\"actual_side\"].notna() & merged[\"side\"].notna() & (merged[\"actual_side\"]!=\"Push\")\n",
    "if mask.any():\n",
    "    merged[\"hit\"] = (merged.loc[mask,\"actual_side\"] == merged.loc[mask,\"side\"]).astype(int)\n",
    "    hit = merged.loc[mask,\"hit\"].mean()\n",
    "    print(f\"[hit-rate] {hit:.3f} on {int(mask.sum())} graded rows\")\n",
    "else:\n",
    "    print(\"[hit-rate] 0 graded rows (likely missing market mapping or side/line)\")\n",
    "\n",
    "# Optional: small by-market summary you can print or to_csv\n",
    "by_mkt = (merged[mask]\n",
    "          .groupby(\"market_std\")[\"hit\"]\n",
    "          .agg(rows=\"count\", hit_rate=\"mean\")\n",
    "          .sort_values([\"hit_rate\",\"rows\"], ascending=[False, False]))\n",
    "print(\"\\n[by-market]\")\n",
    "print(by_mkt.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de3945-53b5-46cf-a919-e7a1f6345de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
